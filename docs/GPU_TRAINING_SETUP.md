# BEVNeXt-SAM2 GPU Training Setup Guide

This guide explains how to set up and run BEVNeXt-SAM2 training with CUDA support for maximum performance.

## Prerequisites

Before setting up GPU training for BEVNeXt-SAM2, ensure you have:

1. **NVIDIA GPU** with CUDA Compute Capability 6.0 or higher
2. **NVIDIA drivers** installed (version 470.57.02 or later)
3. **Docker** installed on your host system

## Step 1: Install NVIDIA Drivers (Host System)

```bash
# Check if drivers are already installed
nvidia-smi

# If not installed, install NVIDIA drivers:
# Ubuntu/Debian:
sudo apt update
sudo apt install -y nvidia-driver-530  # or latest version
sudo reboot

# After reboot, verify installation:
nvidia-smi
```

## Step 2: Install Docker (Host System)

```bash
# Install Docker if not already installed
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# Log out and back in, then verify:
docker --version
```

## Step 3: Install NVIDIA Container Toolkit (Host System)

### For Ubuntu 20.04/22.04/24.04:

```bash
# Add NVIDIA repository
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# Install NVIDIA Container Toolkit
sudo apt update
sudo apt install -y nvidia-container-toolkit

# Configure Docker daemon
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### For CentOS/RHEL/Fedora:

```bash
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | \
    sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

sudo yum install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

## Step 4: Test GPU Access in Docker

```bash
# Test basic GPU access
docker run --rm --runtime=nvidia --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi

# Test PyTorch GPU access
docker run --rm --runtime=nvidia --gpus all pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"
```

## Step 5: Build and Run BEVNeXt-SAM2 with GPU Support

```bash
# Clone and navigate to project (if not already done)
cd /path/to/bevnext-sam2

# Build Docker image with GPU support
docker build -t bevnext-sam2:latest .

# Run GPU training
docker run --rm --runtime=nvidia --gpus all \
    --shm-size=8g \
    -v $(pwd):/workspace \
    -v $(pwd)/outputs:/workspace/outputs \
    -v $(pwd)/checkpoints:/workspace/checkpoints \
    bevnext-sam2:latest \
    python training/train_bevnext_sam2.py --config training/config_gpu.json
```

## Alternative: Use Pre-built GPU Docker Image

If you encounter issues with the custom build, you can use a pre-built PyTorch image:

```bash
# Pull NVIDIA PyTorch image
docker pull nvcr.io/nvidia/pytorch:23.10-py3

# Run with your code mounted
docker run --rm --runtime=nvidia --gpus all \
    --shm-size=8g \
    -v $(pwd):/workspace \
    -w /workspace \
    nvcr.io/nvidia/pytorch:23.10-py3 \
    bash -c "pip install -r requirements.txt && python training/train_bevnext_sam2.py --config training/config_gpu.json"
```

## Training Configurations

### GPU Configuration (High Performance)
- **File**: `training/config_gpu.json`
- **BEV Resolution**: 128Ã—128 
- **Batch Size**: 4
- **Image Size**: 384Ã—384
- **Workers**: 8
- **Mixed Precision**: Enabled

### CPU Configuration (Testing/Debug)
- **File**: `training/config_demo.json`
- **BEV Resolution**: 64Ã—64
- **Batch Size**: 1
- **Image Size**: 224Ã—224
- **Workers**: 0

## Quick Start

### 1. GPU Training (Recommended)
```bash
# Start GPU training with default settings
./scripts/train_gpu.sh

# Start training with custom config
./scripts/train_gpu.sh --config training/my_config.json

# Resume from checkpoint
./scripts/train_gpu.sh --resume outputs/training_gpu/checkpoint_best.pth

# Use specific GPU
./scripts/train_gpu.sh --gpu 1

# Disable mixed precision
./scripts/train_gpu.sh --no-mixed-precision
```

### 2. Monitor Training
```bash
# Check training status
./scripts/monitor_training.sh

# View live logs
docker logs -f $(docker ps --filter ancestor=bevnext-sam2:latest --format "{{.ID}}" | head -1)

# Start TensorBoard
tensorboard --logdir outputs/training_gpu/tensorboard --port 6006
```

## Performance Optimization

### Memory Optimization
1. **Batch Size**: Adjust based on GPU memory
   - RTX 3080 (10GB): batch_size = 2-4
   - RTX 3090 (24GB): batch_size = 6-8
   - A100 (40GB): batch_size = 8-12

2. **BEV Resolution**: Balance quality vs memory
   - 64Ã—64: Low memory (~2GB)
   - 128Ã—128: Medium memory (~6GB)
   - 200Ã—200: High memory (~16GB)

3. **Mixed Precision**: Always enable for modern GPUs
   - Reduces memory usage by ~40%
   - Increases training speed by ~20-30%

### Speed Optimization
1. **Model Compilation**: Automatically enabled for PyTorch 2.0+
2. **DataLoader Workers**: Set to 2Ã— number of CPU cores
3. **Pin Memory**: Enabled by default for GPU training

## Training Features

### âœ… Implemented Features
- **Mixed Precision Training**: Automatic loss scaling
- **Model Compilation**: PyTorch 2.0 torch.compile()
- **Gradient Clipping**: Prevents gradient explosion
- **Learning Rate Scheduling**: Cosine annealing
- **Checkpointing**: Automatic save/restore
- **TensorBoard Logging**: Real-time metrics
- **Multi-GPU Ready**: Single GPU implementation

### ðŸ”„ BEVNeXt-SAM2 Architecture
```
Camera Images (6Ã—3Ã—384Ã—384)
    â†“
Camera Backbone (CNN)
    â†“
BEV Projection (Cameraâ†’BEV)
    â†“
SAM2 Feature Fusion
    â†“
BEV Transformer (Self-Attention)
    â†“
3D Object Detection Head
    â†“
Predictions (Classification + Regression + Confidence)
```

## Troubleshooting

### Common Issues

#### 1. CUDA Out of Memory
```bash
# Reduce batch size
"batch_size": 1

# Reduce BEV resolution
"bev_size": [64, 64]

# Enable gradient checkpointing (if implemented)
"gradient_checkpointing": true
```

#### 2. Docker GPU Access Failed
```bash
# Check NVIDIA Docker runtime
docker info | grep nvidia

# Test GPU access
docker run --rm --runtime=nvidia nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi
```

#### 3. Mixed Precision Issues
```bash
# Disable mixed precision
./scripts/train_gpu.sh --no-mixed-precision
```

### Performance Monitoring
```bash
# GPU utilization
nvidia-smi -l 1

# GPU memory usage during training
watch -n 1 'nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits'

# Training metrics
tensorboard --logdir outputs/training_gpu/tensorboard
```

## Expected Performance

### Training Speed (RTX 3090)
- **Batch Size 4**: ~2.5 seconds/batch
- **Batch Size 8**: ~4.8 seconds/batch
- **Mixed Precision**: 20-30% speedup

### Memory Usage (Batch Size 4)
- **BEV 64Ã—64**: ~4GB GPU memory
- **BEV 128Ã—128**: ~8GB GPU memory
- **BEV 200Ã—200**: ~16GB GPU memory

### Convergence
- **Training Loss**: Should decrease steadily
- **Validation Loss**: Should plateau after 20-30 epochs
- **Best Results**: Typically around epoch 40-50

## Custom Configuration

Create your own config file:
```json
{
    "d_model": 256,
    "nhead": 8,
    "num_transformer_layers": 6,
    "num_classes": 10,
    "num_queries": 900,
    "bev_size": [128, 128],
    "image_size": [384, 384],
    "num_cameras": 6,
    "batch_size": 4,
    "learning_rate": 0.0002,
    "weight_decay": 0.0001,
    "num_epochs": 50,
    "num_workers": 8,
    "num_samples": {
        "train": 2000,
        "val": 400
    },
    "output_dir": "/workspace/outputs/training_custom"
}
```

## Next Steps

1. **Data Loading**: Replace synthetic data with real autonomous driving datasets
2. **Multi-GPU**: Implement DistributedDataParallel
3. **Model Architecture**: Fine-tune transformer layers and attention mechanisms
4. **Loss Functions**: Add advanced 3D detection losses
5. **Evaluation**: Implement mAP, NDS, and other 3D detection metrics

## Cloud GPU Alternatives

If local GPU setup is challenging, consider cloud options:

1. **AWS EC2**: P3/P4 instances with Deep Learning AMI
2. **Google Cloud**: GPU-enabled Compute Engine VMs
3. **Azure**: NC-series VMs with NVIDIA GPUs
4. **Paperspace**: GPU cloud with pre-configured ML environments
5. **RunPod**: Affordable GPU cloud for ML training
6. **Lambda Labs**: Dedicated GPU cloud for deep learning

## Support

For additional help:
- Check Docker logs: `docker logs <container_id>`
- Monitor GPU usage: `nvidia-smi -l 1`
- Check memory usage: `docker stats`